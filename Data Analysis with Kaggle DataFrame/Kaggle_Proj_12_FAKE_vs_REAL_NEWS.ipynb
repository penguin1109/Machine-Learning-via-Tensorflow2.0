{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_Proj.12 FAKE vs REAL NEWS",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1Do47RzRj1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pS7YTe-utmD",
        "colab_type": "text"
      },
      "source": [
        "### csv 파일에 tile 과 text의 내용과 함께 해당 메일의 label의 내용이 담겨 있으며 이를 Fake 와 Real News로 구분하는 Notebook을 작성해 보고자 한다.\n",
        "- DNN만 이용해서 해결하기\n",
        "- RNN + DNN 이용하기  \n",
        "이 두가지 모델을 모두 설계해서 어떠한 모델이 제일 학습도가 높은지 확인해 보고자 한다.\n",
        "  - 그렇게 하기 위해서는 다양한 모델에 적용하기 쉽도록 dataset을 만들어 주는 것이 중요하다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgFxcB0ysKMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news = pd.read_csv('/content/drive/My Drive/news.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXAcfuPnsg0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a153e167-da9f-4408-b7aa-863751ee7b3c"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... label\n",
              "0        8476  ...  FAKE\n",
              "1       10294  ...  FAKE\n",
              "2        3608  ...  REAL\n",
              "3       10142  ...  FAKE\n",
              "4         875  ...  REAL\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F31Kj1uEs-Bi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f5b9e89-8b4a-45a4-de48-4431afce2158"
      },
      "source": [
        "len(news)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX2c8An2tAIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "c00d41db-e498-4baf-bc94-e266bb9531e6"
      },
      "source": [
        "news.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6335 entries, 0 to 6334\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  6335 non-null   int64 \n",
            " 1   title       6335 non-null   object\n",
            " 2   text        6335 non-null   object\n",
            " 3   label       6335 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 198.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcWVxbxltSIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "groupedby_label = news.groupby('label')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15tUSWMiuAMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "76112fa3-ad45-4705-89fd-d4973c06c50a"
      },
      "source": [
        "groupedby_label.count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>FAKE</th>\n",
              "      <td>3164</td>\n",
              "      <td>3164</td>\n",
              "      <td>3164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>REAL</th>\n",
              "      <td>3171</td>\n",
              "      <td>3171</td>\n",
              "      <td>3171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  title  text\n",
              "label                         \n",
              "FAKE         3164   3164  3164\n",
              "REAL         3171   3171  3171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZUZV26xury",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "8abf0ba0-aa4b-4e5e-d34d-abfcb1f23146"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... label\n",
              "0        8476  ...  FAKE\n",
              "1       10294  ...  FAKE\n",
              "2        3608  ...  REAL\n",
              "3       10142  ...  FAKE\n",
              "4         875  ...  REAL\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trmYBTNjxOw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_news = news.drop(news.columns[0], axis = 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GETRXjZx9Vr",
        "colab_type": "text"
      },
      "source": [
        "- column 명이 'Unnamed: 0'인 데이터는 불필요하기 때문에 그것을 drop한 dataset을 drop_news라고 새롭게 저장했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7L9Xguub3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_data = drop_news.values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBzr-8S8wWsZ",
        "colab_type": "text"
      },
      "source": [
        "- 훈련의 목적은 마지막에 2개의 class 중 하나로 분류를 할 수 있도록 하는 것이다.\n",
        "- 먼저 csv파일에 있는 fake data와 real data를 따로 구분해서 title과 text문자열을 합해서 리스트에 각각 나누어 담는다.\n",
        "- 그렇게 한 이후에 텍스트 전처리를 진행해야 한다.\n",
        "  - 여기서 말하는 text preprocessing이란 정규식을 이용해서 '\\nl'등과 같은 문자를 공백으로 바꾸고자 한다.\n",
        "  - 사실상 TextVectorization층을 model의 위에 추가하면 알아서 공백에 맞추어서 나누어주고 단어들을 index화 해 주기 때문에 굳이 미리 이 작업을 해 줄 필요는 없다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdZ1jZGLv1St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake, real = [],[]\n",
        "for i in news_data:\n",
        "  if i[-1] == 'FAKE':fake.append(i[0]+i[1])\n",
        "  else:real.append(i[0]+i[1])\n",
        "real = real[:len(fake)]"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gts4yXlSyOpG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "007e03bb-529f-4e41-a969-e46ef8c0aad0"
      },
      "source": [
        "len(fake), len(real)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3164, 3164)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxodRMYhzB7U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7bccb9e7-5016-4e2b-8616-bd742516c6c6"
      },
      "source": [
        "(fake[3])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tehran, USA  \\nI’m not an immigrant, but my grandparents are. More than 50 years ago, they arrived in New York City from Iran. I grew up mainly in central New Jersey, an American kid playing little league for the Raritan Red Sox and soccer for the Raritan Rovers. In 1985, I travelled with my family to our ancestral land. I was only eight, but old enough to understand that the Iranians had lost their liberty and freedom. I saw the abject despair of a people who, in a desperate attempt to bring about change, had ushered in nationalist tyrants led by Ayatollah Khomeini. \\nWhat I witnessed during that year in Iran changed the course of my life. In 1996, at age 19, wanting to help preserve the blessings of liberty and freedom we enjoy in America, I enlisted in the U.S. Navy. Now, with the rise of Donald Trump and his nationalist alt-right movement, I’ve come to feel that the values I sought to protect are in jeopardy. \\nIn Iran, theocratic fundmentalists sowed division and hatred of outsiders — of Westerners, Christians, and other religious minorities. Here in America, the right wing seems to have stolen passages directly from their playbook as it spreads hatred of immigrants, particularly Muslim ones. This form of nationalistic bigotry — Islamophobia — threatens the heart of our nation. When I chose to serve in the military, I did so to protect what I viewed as our sacred foundational values of liberty, equality, and democracy. Now, 20 years later, I’ve joined forces with fellow veterans to again fight for those sacred values, this time right here at home. \\n“Death to America!” \\nAs a child, I sat in my class at the international school one sunny morning and heard in the distance the faint sounds of gunfire and rising chants of “Death to America!” That day would define the rest of my life. \\nIt was Tehran, the capital of Iran, in 1985. I was attending a unique school for bilingual students who had been born in Western nations. It had become the last refuge in that city with any tolerance for Western teaching, but that also made it a target for military fundamentalists. As the gunfire drew closer, I heard boots pounding the marble tiles outside, marching into our building, and thundering down the corridor toward my classroom. As I heard voices chanting “Death to America!” I remember wondering if I would survive to see my parents again. \\nIn a flash of green and black uniforms, those soldiers rushed into our classroom, grabbed us by our shirt collars, and yelled at us to get outside. We were then packed into the school’s courtyard where a soldier pointed his rifle at our group and commanded us to look up. Almost in unison, my classmates and I raised our eyes and saw the flags of our many nations being torn down and dangled from the balcony, then set ablaze and tossed, still burning, into the courtyard. As those flags floated to the ground in flames, the soldiers fired their guns in the air. Shouting, they ordered us — if we ever wanted to see our families again — to swear allegiance to the Grand Ayatollah Khomeini and trample on the remains of the burning symbols of our home countries. I scanned the smoke that was filling the courtyard for my friends and classmates and, horrified, watched them capitulate and begin to chant, “Death to America!” as they stomped on our sacred symbols. \\nI was so angry that, young as I was, I began to plead with them to come to their senses. No one paid the slightest attention to an eight year old and yet, for the first time in my life, I felt something like righteous indignation. I suspect that, born and raised in America, I was already imbued with such a sense of privilege that I just couldn’t fathom the immense danger I was in. Certainly, I was acting in ways no native Iranian would have found reasonable. \\nAcross the smoke-filled courtyard, I saw a soldier coming at me and knew he meant to force me to submit. I spotted an American flag still burning, dropped to my knees, and grabbed the charred pieces from underneath a classmate’s feet. As the soldier closed in on me, I ducked and ran, still clutching my charred pieces of flag into a crowd of civilians who had gathered to witness the commotion. The events of that day would come to define all that I have ever stood for — or against. \\n“Camel Jockey,” “Ayatollah,” and “Gandhi” \\nMy parents and I soon returned to the United States and I entered third grade. More than anything, I just wanted to be normal, to fit in and be accepted by my peers. Unfortunately, my first name, Nader (which I changed to Nate upon joining the Navy), and my swarthy Middle Eastern appearance, were little help on that score, eliciting regular jibes from my classmates. Even at that young age, they had already mastered a veritable thesaurus of ethnic defamation, including “camel jockey,” “sand-nigger,” “raghead,” “ayatollah,” and ironically, “Gandhi” (which I now take as a compliment). My classmates regularly sought to “other-ize” me in those years, as if I were a lesser American because of my faith and ethnicity. \\nYet I remember that tingling in my chest when I first donned my Cub Scout uniform — all because of the American flag patch on its shoulder. Something felt so good about wearing it, a feeling I still had when I joined the military. It seems that the flag I tried to rescue in Tehran was stapled to my heart, or that’s how I felt anyway as I wore my country’s uniform. \\nWhen I took my oath of enlistment in the U.S. Navy, I gave my mom a camera and asked her to take some photos, but she was so overwhelmed with pride and joy that she cried throughout the ceremony and managed to snap only a few images of the carpet. She cried even harder when I was selected to serve as the first Muslim-American member of the U.S. Navy Presidential Ceremonial Honor Guard . On that day, I was proud, too, and all the taunts of those bullies of my childhood seemed finally silenced. \\nBeing tormented because of my ethnicity and religion in those early years had another effect on me. It caused me to become unusually sensitive to the nature of other people. Somehow, I grasped that, if it weren’t for a fear of the unknown, there was an inherent goodness and frail humanity lurking in many of the kids who bullied and harassed me. Often, I discovered, those same bullies could be tremendously kind to their families, friends, or even strangers. I realized, then, that if, despite everything, I could lay myself bare and trust them enough to reach out in kindness, I might in turn gain their trust and they might then see me, too, and stop operating from such a place of fear and hate. \\nThrough patience, humor, and understanding, I was able to offer myself as the embodiment of my people and somehow defang the “otherness” of so much that Americans found scary. To this day, I have friends from elementary school, middle school, high school, and the military who tell me that I am the only Muslim they have ever known and that, had they not met me, their perspective on Islam would have been wholly subject to the prevailing fear-based narrative that has poisoned this country since September 11, 2001. \\nIn 1998, I became special assistant to the Master Chief Petty Officer of the Navy and then, in 1999, I was recruited to serve at the Defense Intelligence Agency. In August 2000, I transferred to the Naval Reserve. \\nIn the wake of 9/11, I began to observe how so many of my fellow Americans were adopting a fundamentalist “us vs. them” attitude towards Muslims and Islam. I suddenly found myself in an America where the scattered insults I had endured as a child took on an overarching and sinister meaning and form, where they became something like an ideology and way of life. \\nBy the time I completed my military service in 2006, I had begun to understand that our policies in the Middle East,similarly disturbed, seemed in pursuit of little more than perpetual warfare. That, in turn, was made possible by the creation of a new enemy: Islam — or rather of a portrait, painted by the powers-that-be, of Islam as a terror religion, as a hooded villain lurking out there somewhere in the desert, waiting to destroy us. I knew that attempting to dispel, through the patient approach of my childhood, the kind of Islamophobia that now had the country by the throat was not going to be enough. Post-9/11 attacks on Muslims in the U.S. and elsewhere were not merely childish taunts. \\nFor the first time in my life, in a country gripped by fear, I believed I was witnessing a shift, en masse, toward an American fundamentalism and ultra-nationalism that reflected a wanton lack of reason, not to mention fact. As a boy in Iran, I had witnessed the dark destination down which such a path could take a country. Now, it seemed to me, in America’s quest to escape the very demons we had sown by our own misadventures in the Middle East, and forsaking the hallmarks of our founding, we risked becoming everything we sought to defeat. \\nThe Boy in the Schoolyard Grown Up \\nOn February 10, 2015, three young American students, Yusor Abu-Salha, Razan Abu-Salha, and Deah Shaddy Barakat, were executed at an apartment complex in Chapel Hill, North Carolina. The killer was a gun-crazy white man filled with hate and described by his own daughter as “a monster.” Those assassinations struck a special chord of sorrow and loss in me. My mom and I cried and prayed together for those students and their families. \\nThe incident in Chapel Hill also awoke in me some version of the righteous indignation I had felt so many years earlier in that smoke-filled courtyard in Iran. I would be damned if I stood by while kids in my country were murdered simply because of their faith. It violated every word of the oath I had taken when I joined the military and desecrated every value I held in my heart as a sacred tenet of our nation. White nationalists and bigots had, by then, thrown down the gauntlet for so much of this, using Islamophobia to trigger targeted assassinations in the United States. This was terrorism, pure and simple, inspired by hate-speakers here at home. \\nAt that moment, I reached out to fellow veterans who, I thought, might be willing to help — and it’s true what they say about soul mates being irrevocably drawn to each other. When I contacted Veterans For Peace , an organization dedicated to exposing the costs of war and militarism, I found the leadership well aware of the inherent dangers of Islamophobia and of the need to confront this new enemy. So Executive Director Michael McPhearson formed a committee of vets from around the country to decide how those of us who had donned uniforms to defend this land could best battle the phenomenon — and I, of course, joined it. \\nFrom that committee emerged Veterans Challenge Islamophobia (VCI). It now has organizers in Arizona, Georgia, New Jersey, and Texas, and that’s just a beginning. Totally nonpartisan, VCI focuses on politicians of any party who engage in hate speech. We’ve met with leaders of American Muslim communities, sat with them through Ramadan, and attended their Iftar dinners to break our fasts together. In the wake of the Orlando shooting , we at VCI also mobilized to fight back against attempts to pit the Muslim community against the LGBTQ+ community. \\nOur group was born of the belief that, as American military veterans, we had a responsibility to call out bigotry, hatred, and the perpetuation of endless warfare. We want the American Muslim community to know that they have allies, and that those allies are indeed veterans as well. We stand with them and for them and, for those of us who are Muslim, among them. \\nNationalism and xenophobia have no place in American life, and I, for my part, don’t think Donald Trump or anyone like him should be able to peddle Islamophobia in an attempt to undermine our national unity. Without Islamophobia, there no longer exists a “clash of civilizations.” Without Islamophobia, whatever the problems in the world may be, there is no longer an “us vs. them” and it’s possible to begin reimagining a world of something other than perpetual war. \\nAs of now, this remains the struggle of my life, for despite my intense love for America, some of my countrymen increasingly see American Muslims as the “other,” the enemy. \\nMy Mom taught me as a boy that the only thing that mattered was what was in my heart. Now, with her in mind and as a representative of VCI, when I meet fellow Americans I always remember my childhood experiences with my bullying peers. And I still lay myself bare, as I did then. I give trust to gain trust, but always knowing that these days this isn’t just a matter of niceties. It’s a question of life or death. It’s part of a battle for the soul of our nation. \\nIn many ways, I still consider myself that boy in the school courtyard in Tehran trying to rescue charred pieces of that flag from those trampling feet. It’s just that now I’m doing it in my own country. \\nNate Terani is a veteran of the U.S. Navy and served in military intelligence with the Defense Intelligence Agency. He is currently a member of the leadership team at Common Defense PAC and regional campaign organizer with Veterans Challenge Islamophobia . He is a featured columnist with the Arizona Muslim Voice newspaper. (Reprinted from TomDispatch by permission of author or representative)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvAkjfBN1Bsp",
        "colab_type": "text"
      },
      "source": [
        "- 간단하게 preprocess 함수를 만들어 보자면 아래와 같다.\n",
        "  - 정규화 조건을 이용했기 때문에 우선 문자가 아닌 값들은(알파벳이 아닌 값) 모두 공백으로 처리 했고, '\\n'과 같은 띄어쓰기 등을 의미하는 값들 또한 모두 공백으로 바꾸어 주었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2nHIzcV0JbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_fake(x):\n",
        "  x = tf.strings.substr(x, 0, 300)\n",
        "  x = tf.strings.regex_replace(x, b\"<br\\\\s*/?>\", b\" \")\n",
        "  x = tf.strings.regex_replace(x, b\"[^a-zA-Z']\", b\" \")\n",
        "  x = tf.strings.split(x)\n",
        "  return x.to_tensor(default_value = b\"<pad>\"), tf.constant([0])\n",
        "\n",
        "def preprocess_real(x):\n",
        "  x = tf.strings.substr(x, 0, 300)\n",
        "  x = tf.strings.regex_replace(x, b\"<br\\\\s*/?>\", b\" \")\n",
        "  x = tf.strings.regex_replace(x, b\"[^a-zA-Z']\", b\" \")\n",
        "  x = tf.strings.split(x)\n",
        "  return x.to_tensor(default_value = b\"<pad>\"), tf.constant([1])"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSD6wej41YVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "fake_train, fake_test = train_test_split(fake, test_size = 0.2)\n",
        "fake_train, fake_val = train_test_split(fake_train, test_size = 0.2)\n",
        "real_train, real_test = train_test_split(real, test_size = 0.2)\n",
        "real_train, real_val = train_test_split(real_train, test_size = 0.2)"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9yUhhM12Py5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03a4c9f1-1e83-4654-9ac7-b0d40adc48c0"
      },
      "source": [
        "len(real_train), len(fake_test), len(fake_val)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2024, 633, 507)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3LWiOkK4mN5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0070fb25-edd7-48ff-de43-c6ab1e5c5b9a"
      },
      "source": [
        "fake_train[0]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Valentin Katasonov: America is in agony and Trump is the doctor\\n\\nNovember 12, 2016 - Fort Russ \\n\\nNeyromir TV (Video)  - Valentin Katasonov - Translated from Russian by Kristina Kharlova \\n\\n\\nThe heated discussion in the media is a diversion - focused on marginal issues appealing to emotions, while much graver issues that are at stake are hidden behind the scenes, explains Valentin Katasonov, p rofessor, associate member of the Russian Academy of Economic Science and Business. \\n\\n\\n\\n\\nPart 1 (00.00-14.00) \\n\\n\\n\\n\\n\\nV.K: Trump understands the situation. I didn't expect him to be so open about revealing all the ills. He is revealing many of the secrets. This is better for America - to face the diagnosis, than to conceal it from  the patient.  \\n\\nAs you know interest rates in the countries of the Golden billion are below the floor.  Last year when interest rates were slightly raised, this caused serious consequences . Christine Lagarde appealed to stop or the global economy will crash. They did not raise interest rates again as planned. Not in the first or the second quarter. They are in a very difficult situation. Trump said if interest rates are raised America will default, because most of the budget will go to pay for interest rates.  \\n\\nTrump probably wants to save capitalism, but capitalism without interest rates is nonsense. Trump  is a defender of capitalism. \\n\\nHost: Who is Hillary Clinton? \\n\\nV.K.: She doesn't say who she is, she is focused on some marginal subjects, like rights of minorities or climate change. We understand that these are silly games. \\n\\nHost: But climate change is real? \\n\\nV.K.: You know I was involved in this as part of World Bank and I know their schemes. Although there might be something going on, I can assure that they don't really care about it.  \\n\\nHost: What about the ruble? \\n\\nV.K. It depends when this apocalypse will take place. The dropping of the ruble is part of the plan of the occupation. \\n\\nOnly God knows when this apocalypse will happen, may be not next week, could be few months of years for sure, because all the resources have been exhausted. Trump senses this very well, and he wants to save capitalism and America, and for this he wants to negotiate with the lenders and restructure American debt. In reality America is working on new technologies which don't require any negotiations, I am taking about Iran. As you know in the beginning of 2o16 Uncle Sam said we are finally cancelling sanctions and unfreeze Iran's foreign assets. \\n\\nThere are many conditions, some things are written down. Iran is not happy that America is writing off $2 billion to cover losses from terrorist act in Lebanon in 1983. I looked at some documents, Iran had nothing to do with it, but Uncle Sam found them guilty, just like those in 9/11. It turns out the culprit was Saudi Arabia.  \\n\\nHost: But Saudi Arabia threatened to sell treasuries and was taken off the list of culprits? \\n\\nV.K. Who will let them sell it? Treasuries are in the depositories, it's a double key system, Saudi Arabia has one key and Uncle Sam has another key \\n\\nHost: Why is Russia buying these treasuries while in a crisis? We did mention this is a levy, but if we don't develop our industry and science nothing will help us? \\n\\nV.K. You know some of our elites hope they have an second base in the USA, the 'unsinkable'. We know some people have already set up base there, like the first deputy minister of finance. Some are closer, like the former minister of agriculture is in France. It is easier to count those who did not leave. This shows we are dealing with colonial administration which receives basic guarantees for citizenship in the US, France, GB. \\n\\nHost: Their policies lead to total collapse, what if the owners are not happy? \\n\\nV.K.: You know, lets not overestimate the masters, they act as parasites. Parsites keep feeding not thinking that the food will run out and they may also perish. They don't see beyond their nose. \\n\\nSpeaking of Iran, according to Iran, their foreign reserves comprise $130 billion, according to US - $100 billion, half of them are gold and currency reserves belonging to Iran's Central bank, or the Sovereign fund of Iran. I think they will eat up these 130 billion very fast, because after the $2 billion, US announced they will demand $11.5 billion for 9/11. The appetite comes during the meal. The Iranian parliament discussed a bill to allow the government to begin a case about damages to Iran by USA. A working group will inventory all the events and estimate them.  \\n\\nToday the world is entering a repatriation game, this is a commercialization of international relations and monetizing of our history. This is very important to Russian Federation. Baltic countries continue to work on such demands, especially Latvia. Ukraine is a little different. While the Baltics refer to Soviet occupation, Ukraine is talking about Crimea. Monetizing losses from events in Donbass and so on. I am just saying we should be a step ahead, we must prepare the same contra-measures. \\n\\n\\n\\nDon't get caught up in the media's agenda, dig deeper and think with your own head! - KK \\n\\n\\n\\nPart 2 coming soon... \\n\\n\\n     Follow us on Facebook!                                                   \\n                                                   \\n\\n       Follow us on Twitter! \\n                               \\n\\n             Donate! \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3kTQ96cUdQA",
        "colab_type": "text"
      },
      "source": [
        "DATA 1-1. DNN Layer을 위한 data 만들기\n",
        "- 어차피 TextVectorization을 이용할 것이기 때문에 따로 전처리를 진행한 것은 아니고 그냥 x, y를 반한하도록 했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXXDpSDJzzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_dnn_fake(x):\n",
        "  return x,0\n",
        "def preprocess_dnn_real(x):\n",
        "  return x,1\n",
        "\n",
        "fake_train_dnn = tf.data.Dataset.from_tensor_slices(fake_train).map(preprocess_dnn_fake)\n",
        "fake_test_dnn = tf.data.Dataset.from_tensor_slices(fake_test).map(preprocess_dnn_fake)  \n",
        "fake_val_dnn = tf.data.Dataset.from_tensor_slices(fake_val).map(preprocess_dnn_fake)  \n",
        "\n",
        "\n",
        "real_train_dnn = tf.data.Dataset.from_tensor_slices(real_train).map(preprocess_dnn_real)\n",
        "real_test_dnn = tf.data.Dataset.from_tensor_slices(real_test).map(preprocess_dnn_real)  \n",
        "real_val_dnn = tf.data.Dataset.from_tensor_slices(real_val).map(preprocess_dnn_real)  \n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v87hI8uUKyIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dnn = tf.data.Dataset.concatenate(fake_train_dnn, real_train_dnn).repeat().shuffle(2024).batch(32)\n",
        "test_dnn = tf.data.Dataset.concatenate(fake_test_dnn, real_test_dnn).batch(32)\n",
        "val_dnn = tf.data.Dataset.concatenate(fake_val_dnn, real_val_dnn).batch(32)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX9m6eMYicXw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c6c9799-9e79-422f-932d-35f8ab049c34"
      },
      "source": [
        "for i,j in train_dnn.take(1):\n",
        "  print(j)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(32,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGGIbnEyUpQc",
        "colab_type": "text"
      },
      "source": [
        "DATA 1-2. DNN + RNN Layer을 위한 data 만들기\n",
        "- preprocess 함수를 이용해서 미리 데이터의 적재와 전처리를 해준다.\n",
        "- 알파벳 제외 다른 것들은 공백으로 남겨 두는데, 무조건 마지막에 모든 텍스트 데이터의 길이를 동일하게 설정하기 위해서 dafault_value는 <pad>로 바꿔주는 과정을 거친다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK3huTCV30M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "fake_train = tf.data.Dataset.from_tensor_slices(fake_train).batch(BATCH_SIZE).map(preprocess_fake)\n",
        "fake_test = tf.data.Dataset.from_tensor_slices(fake_test).batch(BATCH_SIZE).map(preprocess_fake)\n",
        "fake_val = tf.data.Dataset.from_tensor_slices(fake_val).batch(BATCH_SIZE).map(preprocess_fake)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfv4GyyWkEJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64005def-45b2-4e70-a0a4-cc213286b972"
      },
      "source": [
        "for x,y in fake_train.take(1):\n",
        "  print(y)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0], shape=(1,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mngb5rqD5UiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_train = tf.data.Dataset.from_tensor_slices(real_train).batch(BATCH_SIZE).map(preprocess_real)\n",
        "real_test = tf.data.Dataset.from_tensor_slices(real_test).batch(BATCH_SIZE).map(preprocess_real)\n",
        "real_val = tf.data.Dataset.from_tensor_slices(real_val).batch(BATCH_SIZE).map(preprocess_real)"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "539xKVtk7qAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 2024\n",
        "train_dataset = tf.data.Dataset.concatenate(fake_train, real_train).shuffle(BUFFER_SIZE)\n",
        "test_dataset = tf.data.Dataset.concatenate(fake_test, real_test)\n",
        "val_dataset = tf.data.Dataset.concatenate(fake_val, real_val)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBqeap6D-i6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45302a09-e141-4fea-d2e8-c86b5b955d18"
      },
      "source": [
        "for x,y in train_dataset.take(1):\n",
        "  print(y)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0], shape=(1,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcDeMB4n-MPb",
        "colab_type": "text"
      },
      "source": [
        "### 1. RNN + DNN Layer\n",
        "- 이 방법으로 하기 위해서 직접 단어 사전을 만들어서 단어를 인코딩을 했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEzopf6S-RCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "vocab = Counter()\n",
        "for x, y in train_dataset:\n",
        "  for text in x:\n",
        "    vocab.update(list(text.numpy()))"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDO5ljJZgS3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train_dataset.repeat()"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puiJ2pkWAmt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9350c730-bb68-420e-bd4d-b628bca13e45"
      },
      "source": [
        "vocab.most_common()[:5]"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(b'<pad>', 38896), (b'the', 7443), (b'to', 4256), (b'of', 4114), (b'a', 3535)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss4AqYa8DD9y",
        "colab_type": "text"
      },
      "source": [
        "**주어진 텍스트를 이용해서 단어 사전을 만드는 과정**  \n",
        "\n",
        "1. 먼저 Counter이라는 함수를 불러서 해당 텍스트 데이터에 있는 모든 단어를 dictionaty의 형태에 넣어 각 단어가 몇번 이 나왔는지 자동으로 저장해 준다.\n",
        "2. 그 단어 사전에 저장된 단어들 중 우리가 유효하게 의미를 생각할 단어의 개수를 정해주고 most_common함수를 이용해서 가장 많이 쓰인 것만 저장을 따로 해준다.\n",
        "3. tensor의 형태로 바꾸어 준 뒤에 개수만큼 정수가 나열된 list를 만든다.\n",
        "4. 이후 ```tf.lookup.KeyValueTensorInitializer```을 이용해서 단어와 단어의 인덱스를 match해주는 lookup 도구를 만든다.\n",
        "5. 마지막으로 혹시 모를 사전에 없을 단어들을 위해서 num_oov_bucket까지 추가 해 주어 학습에 사용될 데이터에 적용될 lookup table을 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAhyr_nwAqtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "vocab_dict = [word for word, count in vocab.most_common()[:vocab_size]]\n",
        "\n",
        "words = tf.constant(vocab_dict)\n",
        "words_ind = tf.range(len(vocab_dict), dtype = tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, words_ind)\n",
        "num_oov_bucket = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_bucket)\n",
        "\n",
        "def encode_word(x,y):\n",
        "  return table.lookup(x), y\n",
        "\n",
        "train_data = train_dataset.map(encode_word)\n",
        "test_data = test_dataset.map(encode_word)\n",
        "val_data = val_dataset.map(encode_word)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh4MbVPzgreS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "fc173ddf-ce0b-4d05-90a6-f989505648b2"
      },
      "source": [
        "for i,j in train_data.take(1):print(i,j)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   93    54  1748 ...     0     0     0]\n",
            " [  859    41  1635 ...     0     0     0]\n",
            " [ 3847    30  2560 ...     0     0     0]\n",
            " ...\n",
            " [10186  6272 10527 ...     0     0     0]\n",
            " [  180  1839  1039 ...     0     0     0]\n",
            " [  119   448  3413 ...     0     0     0]], shape=(32, 54), dtype=int64) tf.Tensor([1], shape=(1,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryyIoqutB9WS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "f1df2aca-7e9a-4145-86e5-e90269b00184"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "\n",
        "embed_size = 128\n",
        "model = Sequential()\n",
        "model.add(Embedding(11000, embed_size, input_shape = [None]))\n",
        "model.add(GRU(128, return_sequences = True))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(100, activation = 'selu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, None, 128)         1408000   \n",
            "_________________________________________________________________\n",
            "gru_15 (GRU)                 (None, None, 128)         99072     \n",
            "_________________________________________________________________\n",
            "gru_16 (GRU)                 (None, 128)               99072     \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,619,145\n",
            "Trainable params: 1,619,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEq0OPOPk5J_",
        "colab_type": "text"
      },
      "source": [
        "- 이 모델에 fitting 할 때에 처음에는 출력층의 shape와 label의 shape가 일치 하지 않았어서 문제가 많이 발생했었다. \n",
        "- 그래서 보니까 dataset에 저장된 label이 shape = ()으로 None으로 지정이 되어 있음을 알 수 있었다. 그래서 preprocessing 함수에서 y의 값을 그냥 상수가 아니라 [0]과 [1]로 반환해 주었더니 shape가 생겼기 때문에 문제 없이 모델을 학습할 수 있었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hFgeOejEW1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "b4e595b5-b8c0-47c7-e7fe-fb9a02f30486"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
        "history = model.fit(train_data, validation_data = val_data, epochs = 10, steps_per_epoch = 2024//BATCH_SIZE)"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 10s 156ms/step - loss: 0.7037 - accuracy: 0.4573 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 9s 143ms/step - loss: 0.6963 - accuracy: 0.4675 - val_loss: 0.6942 - val_accuracy: 0.5079\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 9s 141ms/step - loss: 0.7267 - accuracy: 0.5574 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 9s 143ms/step - loss: 0.7085 - accuracy: 0.5243 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 9s 146ms/step - loss: 0.6878 - accuracy: 0.5308 - val_loss: 0.6853 - val_accuracy: 0.5108\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 9s 144ms/step - loss: 0.5808 - accuracy: 0.7282 - val_loss: 0.6304 - val_accuracy: 0.7041\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 9s 144ms/step - loss: 0.3564 - accuracy: 0.8674 - val_loss: 0.4815 - val_accuracy: 0.7771\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 9s 145ms/step - loss: 0.2370 - accuracy: 0.9167 - val_loss: 0.3399 - val_accuracy: 0.8659\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 9s 143ms/step - loss: 0.0763 - accuracy: 0.9769 - val_loss: 0.3800 - val_accuracy: 0.8669\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 9s 145ms/step - loss: 0.0744 - accuracy: 0.9789 - val_loss: 0.3920 - val_accuracy: 0.8925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzkO4mWIk_lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8b72b54b-80fd-4232-8d49-3d91a663374a"
      },
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 1s 32ms/step - loss: 0.3338 - accuracy: 0.9028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33382105827331543, 0.9028435945510864]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbSbmMKGlEDM",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy = 90.28%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRTnPfC28xdx",
        "colab_type": "text"
      },
      "source": [
        "### 2. DNN Layer Only\n",
        "- tf.keras.layers.TextVectorzation 층을 이용해서 범주형 데이터를 수치형 데이터로 바꾸어 준다.\n",
        "- 그리고 사용자 정의 Standardization 층을 만들어서 이렇게 인덱스된 데이터의 각 단어별 id를 만들어 주고자 한다.\n",
        "  - TextVectorization을 안 사용한다면 직접 해당 텍스트 데이터에 있는 단어들을 이용해서 단어 사전을 만들어야 할 것이다.\n",
        "\n",
        "- 반드시 모델의 loss는 binary_crossentropy여야 한다. categorical_crossentropy는 class의 개수가 더 많을 때에 사용한다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzAhJBlK7wDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "vect_layer = TextVectorization(max_tokens = 11000, output_mode = 'int', output_sequence_length = 500)\n",
        "vect_layer.adapt(test_dnn.map(lambda x,y:x))"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8KY35HhNWpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "bc46a7ea-c3e6-406f-ce04-cdcbe3b62773"
      },
      "source": [
        "for i,j in test_dnn.take(1):\n",
        "  print(vect_layer(i))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[5486    1  358 ...   45 2121    3]\n",
            " [ 225 9066   54 ...   27   44    4]\n",
            " [ 164   37  922 ...   26    8    1]\n",
            " ...\n",
            " [2119 1276  168 ...    3   68 8673]\n",
            " [ 164  132 1308 ...  953 5897  416]\n",
            " [1725 4816    9 ...    0    0    0]], shape=(32, 500), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKwmbqHlLqUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BagOfWords(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_tokens, dtype = tf.int32):\n",
        "    super().__init__(dtype = tf.int32)\n",
        "    self.n_tokens = n_tokens\n",
        "  def call(self, inputs):\n",
        "    one_hot = tf.one_hot(inputs, self.n_tokens)\n",
        "    return tf.reduce_sum(one_hot, axis = 1)[:, 1:]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq2pEPbrMK-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bag_of_words = BagOfWords(500)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJWJl9lSQmj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "9d73c14a-6eb1-4708-c8ef-f2b42888c698"
      },
      "source": [
        "for i,j in test_dnn.take(1):\n",
        "  print(vect_layer(i))\n",
        "  print(bag_of_words(vect_layer(i)))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    2 10297     1 ...   164  1272     2]\n",
            " [    2  1675     1 ...     0     0     0]\n",
            " [   89     3   389 ...  1824    76  3177]\n",
            " ...\n",
            " [    2     1     7 ...   396    31    17]\n",
            " [  425     1   149 ...     0     0     0]\n",
            " [   35    36   664 ...     0     0     0]], shape=(32, 500), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[54. 33. 15. ...  0.  0.  0.]\n",
            " [34. 43. 10. ...  0.  0.  0.]\n",
            " [44. 14. 29. ...  0.  0.  0.]\n",
            " ...\n",
            " [47. 29. 11. ...  0.  0.  0.]\n",
            " [41. 17. 12. ...  0.  0.  1.]\n",
            " [ 6. 10.  4. ...  0.  0.  0.]], shape=(32, 499), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1fyIouLS10c",
        "colab_type": "text"
      },
      "source": [
        "**아래 모델을 설계할 때에 반드시 class가 2개이기 때문에 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4caSZqCUHPKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "85aae953-c00d-47bc-d437-6a6d2693f31b"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(vect_layer)\n",
        "model.add(bag_of_words)\n",
        "model.add(Dense(100, activation = 'relu'))\n",
        "model.add(Dense(300, activation = 'relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
        "history = model.fit(train_dnn, validation_data= val_dnn, epochs = 20, steps_per_epoch= 2024//32)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 3s 44ms/step - loss: 0.5334 - accuracy: 0.7307 - val_loss: 0.3947 - val_accuracy: 0.8097\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 3s 40ms/step - loss: 0.3789 - accuracy: 0.8343 - val_loss: 0.4331 - val_accuracy: 0.7850\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 3s 41ms/step - loss: 0.3743 - accuracy: 0.8398 - val_loss: 0.3434 - val_accuracy: 0.8560\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 3s 41ms/step - loss: 0.3564 - accuracy: 0.8557 - val_loss: 0.3585 - val_accuracy: 0.8521\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 3s 41ms/step - loss: 0.3327 - accuracy: 0.8522 - val_loss: 0.3045 - val_accuracy: 0.8728\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 3s 41ms/step - loss: 0.2728 - accuracy: 0.8874 - val_loss: 0.3212 - val_accuracy: 0.8600\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 3s 43ms/step - loss: 0.3011 - accuracy: 0.8740 - val_loss: 0.3740 - val_accuracy: 0.8304\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 3s 43ms/step - loss: 0.2574 - accuracy: 0.8943 - val_loss: 0.3659 - val_accuracy: 0.8560\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 3s 44ms/step - loss: 0.2911 - accuracy: 0.8864 - val_loss: 0.5880 - val_accuracy: 0.7475\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 3s 47ms/step - loss: 0.2171 - accuracy: 0.9137 - val_loss: 0.5199 - val_accuracy: 0.8215\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 3s 45ms/step - loss: 0.1932 - accuracy: 0.9236 - val_loss: 0.3222 - val_accuracy: 0.8639\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 3s 43ms/step - loss: 0.2149 - accuracy: 0.9097 - val_loss: 0.5346 - val_accuracy: 0.8284\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 3s 44ms/step - loss: 0.1913 - accuracy: 0.9211 - val_loss: 0.3653 - val_accuracy: 0.8600\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 3s 45ms/step - loss: 0.1686 - accuracy: 0.9335 - val_loss: 0.4789 - val_accuracy: 0.8393\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 3s 43ms/step - loss: 0.1594 - accuracy: 0.9345 - val_loss: 0.3833 - val_accuracy: 0.8501\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 3s 43ms/step - loss: 0.1120 - accuracy: 0.9608 - val_loss: 0.4109 - val_accuracy: 0.8738\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 3s 47ms/step - loss: 0.1618 - accuracy: 0.9380 - val_loss: 0.4160 - val_accuracy: 0.8471\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 3s 44ms/step - loss: 0.0949 - accuracy: 0.9678 - val_loss: 0.4637 - val_accuracy: 0.8787\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 3s 44ms/step - loss: 0.1266 - accuracy: 0.9524 - val_loss: 0.4946 - val_accuracy: 0.8363\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 3s 44ms/step - loss: 0.0729 - accuracy: 0.9707 - val_loss: 0.5780 - val_accuracy: 0.8402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWLQRx46MZ6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "525bb8a8-e545-4f47-a4af-1af27e72a413"
      },
      "source": [
        "model.evaluate(test_dnn)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 1s 26ms/step - loss: 0.4078 - accuracy: 0.8728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.40784668922424316, 0.8728278279304504]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvR8aFcAlIlM",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy = 87.28%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVa3Ii5nOd4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dnn = test_dnn.shuffle(100)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qk6nsSDOis-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "ecf20d43-3b8d-4722-dc93-2f33bb517e78"
      },
      "source": [
        "for i, j in test_dnn.take(20):\n",
        "  pred = model.predict(i)\n",
        "  if pred[0] < 1-pred[0]:print((j[0].numpy(), 0))\n",
        "  else:print((j[0].numpy(), 1))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n",
            "(0, 1)\n",
            "(0, 0)\n",
            "(0, 0)\n",
            "(1, 1)\n",
            "(0, 1)\n",
            "(0, 0)\n",
            "(0, 0)\n",
            "(0, 0)\n",
            "(0, 0)\n",
            "(0, 1)\n",
            "(1, 0)\n",
            "(1, 1)\n",
            "(1, 1)\n",
            "(0, 0)\n",
            "(1, 1)\n",
            "(1, 0)\n",
            "(0, 0)\n",
            "(1, 1)\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWRaNO9Lmxh3",
        "colab_type": "text"
      },
      "source": [
        "### 결론\n",
        "1. 텍스트 전처리를 할 수 있는 방법은 정말 많지만, 이번에는 두가지 방법을 진행해 보았다.\n",
        "  - 아무래도 문자 사이의, 그리고 문맥 사이의 유사도를 찾아서 indexing해주는 Embeddding layer을 포함한 RNN layer의 정확도가 더 높았다.\n",
        "  - 진행한 두가지 방법은 \n",
        "    1. 먼저 불필요한 문자나 여백 등은 제거하고 vocab lookup set를 직접 만들고 이를 적용해서 문장들을 인덱싱 해준 후에 Embedding Layer과 GRU순환 신경망을 이용해서 단어사이의 유사도를 탐색해 학습을 진행한다.\n",
        "    2. 전처리는 따로 해 주지 않고 label과 mapping만 해 준 이후에 TextVectorization과 BoxOfWords를 이용해서 학습을 해 준다. 이때는 심층 신경망만 이용을 했다.\n",
        "2. 아무래도 2개의 class로 나누는 것이다 보니까 loss를 반드시 'binary_crossentropy'로 설정 해 주어야만 했다.    "
      ]
    }
  ]
}