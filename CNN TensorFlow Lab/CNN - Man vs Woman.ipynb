{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 1859 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, fill_mode = 'nearest')\n",
    "test_data = test_datagen.flow_from_directory('C:/faces94/train', target_size = (150,150), batch_size = 5, class_mode = 'binary')\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, fill_mode = 'nearest')\n",
    "train_data = train_datagen.flow_from_directory('C:/faces94/test', target_size = (150,150), batch_size = 5, class_mode = 'binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬에서 다운 받은 여자와 남자의 얼굴 사진 데이터를 직접 test와 train 데이터로 분류하여 전처리까지 하였다.\n",
    "또한, 두개의 class로 구분되어야 하기 때문에 class_mode = 'binary'로 설정하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), input_shape = (150,150,3), activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 모델을 설계하였는데, 일반적인 컨볼루션 모델처럼\n",
    "convolution layer + pooling layer + flatten layer + dense layer로 구성하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00002), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0과 0.1로 이미지가 구성되도록 255.0으로 나누어서 전처리를 하였으며, 그랬기 때문에 sparse_categorical_crossentropy를 이용해서 손실을 구하였다. 그리고 learning_rate를 0.00002로 설정하였을 때 더욱더 학습이 잘 되는 듯 보였다.\n",
    "무엇보다 분류문제에서 제일 중요한 것은 정확도이기 때문에 metrics값으로 'accuracy'를 설정해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 0.4935 - accuracy: 0.8260 - val_loss: 0.3400 - val_accuracy: 0.8500\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 0.4058 - accuracy: 0.8440 - val_loss: 0.3433 - val_accuracy: 0.8500\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.3724 - accuracy: 0.8677 - val_loss: 0.3360 - val_accuracy: 0.8500\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 14s 142ms/step - loss: 0.3484 - accuracy: 0.8760 - val_loss: 0.3300 - val_accuracy: 0.8500\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 0.3746 - accuracy: 0.8597 - val_loss: 0.3369 - val_accuracy: 0.8500\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.3188 - accuracy: 0.8780 - val_loss: 0.3346 - val_accuracy: 0.8500\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 15s 151ms/step - loss: 0.3612 - accuracy: 0.8580 - val_loss: 0.3567 - val_accuracy: 0.8500\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.3590 - accuracy: 0.8540 - val_loss: 0.3368 - val_accuracy: 0.8500\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 14s 145ms/step - loss: 0.3055 - accuracy: 0.8800 - val_loss: 0.3302 - val_accuracy: 0.8500\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.3269 - accuracy: 0.8537 - val_loss: 0.3342 - val_accuracy: 0.8500\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 0.3273 - accuracy: 0.8660 - val_loss: 0.3105 - val_accuracy: 0.8500\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.3505 - accuracy: 0.8300 - val_loss: 0.3432 - val_accuracy: 0.8500\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.3045 - accuracy: 0.8660 - val_loss: 0.3249 - val_accuracy: 0.8500\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 0.2698 - accuracy: 0.8737 - val_loss: 0.3488 - val_accuracy: 0.8500\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 0.2480 - accuracy: 0.8960 - val_loss: 0.3166 - val_accuracy: 0.9000\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.2532 - accuracy: 0.8980 - val_loss: 0.3584 - val_accuracy: 0.9000\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 0.2631 - accuracy: 0.8880 - val_loss: 0.3617 - val_accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.2786 - accuracy: 0.8818 - val_loss: 0.3221 - val_accuracy: 0.9000\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.2344 - accuracy: 0.9040 - val_loss: 0.3769 - val_accuracy: 0.8500\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 0.2280 - accuracy: 0.9058 - val_loss: 0.3662 - val_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_data, steps_per_epoch = 100, epochs = 20, validation_data = test_data, validation_steps = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 8s 50ms/step - loss: 0.2684 - accuracy: 0.9112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26841962405014785, 0.91125]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 91.125%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
