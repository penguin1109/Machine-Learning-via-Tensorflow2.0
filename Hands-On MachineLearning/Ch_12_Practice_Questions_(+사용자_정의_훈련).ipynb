{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch.12 Practice Questions (+사용자 정의 훈련)",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-XGavAWVY7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BJKMdkVdtG",
        "colab_type": "text"
      },
      "source": [
        "#### 1 . 텐서플로를 한마디로 정의 하자면? 텐서 플로의 주요 특징에는 무엇이 있는가?  \n",
        "- Tensorflow는 강력한 수치 계산용 라이브러리로, 대규모 머신러닝에 잘 맞도록 튜닝이 되어있는 라이브러리이다.\n",
        "- 역전파 기반의 최적화 API롸 tf.keras, tf.data, tf.image, tf.signal등의 API를 제공하기 때문에 딥러닝을 하는데에 있어서 매우 편리할 수 밖에 없다.\n",
        "\n",
        "#### 2. 텐서플로와 넘파이?  \n",
        "- tensorflow가 numpy의 대부분의 기능을 제공하기는 하지만\n",
        "  1. 몇가지의 함수의 이름이 다르고  \n",
        "  2. 일부 함수는 작동 방식이 완전히 동일하지 않으며  \n",
        "  3. numpy array는 변경이 가능하지만 tensorflow의 tensor은 만든 이후에 변경이 불가능하다.\n",
        "  - 만약 변경하고 싶다면 tf.Variable()을 이용해서 변경 가능한 객체를 만들어야 한다.\n",
        "\n",
        "#### 3. tf.range(10) tf.constant(np.arange(10))\n",
        "- 전자와 후자는 모두 0에서 9까지의 정수를 담은 1차원 tensor을 반환한다.\n",
        "- 그러나 전자는 32비트 정수를 사용하고, 후자는 64비트 정수를 사용한다는 차이가 존재한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnuC7-aEYP9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f7baf7c7-6a82-44ca-a0dc-964780bbd158"
      },
      "source": [
        "tf.range(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N9cDQx2Yvpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "508ba2bd-1d7f-46a7-ed24-24ff8e56c7e9"
      },
      "source": [
        "tf.constant(np.arange(10))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts6ppP1QYx4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var = tf.Variable(np.arange(10))\n",
        "cons = tf.constant(np.arange(10))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R5MVex_Y3g8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "990b2b98-d9ad-49f1-c576-185fa4461c84"
      },
      "source": [
        "var[0], cons[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
              " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1nlca1xY8ac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "26e791bb-3d12-48b5-a041-1c20d1cccbd6"
      },
      "source": [
        "var[2].assign(10)\n",
        "var"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(10,) dtype=int64, numpy=array([ 0,  1, 10,  3,  4,  5,  6,  7,  8,  9])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsMe01MVZKk2",
        "colab_type": "text"
      },
      "source": [
        "- 위와 같이 만약에 tensor 형태의 객체를 바꿔주고자 한다면 Variable로 설정해서 **assign()**함수를 이용해 주면 된다.\n",
        "#### 4. 일반 tensor 외에 tensorflow에서 사용할 수 있는 6가지 다른 데이터 구조는?  \n",
        "1. **tf.SparseTensor (희소 텐서)**\n",
        "   - 대부분 0으로 채워진 tensor을 효율적으로 나타내 준다.\n",
        "   - tf.sparse 패키지는 이러한 희소텐서를 위한 연산을 제공해 준다.\n",
        "2. **tf.TensorArray (텐서 배열)**\n",
        "  - tensor의 list이다.\n",
        "  - 기본적으로 고정된 길이를 가지지만 동적으로 바꿀 수 있다.\n",
        "  - 유의할 점은 이 tensorlist에 포함된 모든 tensor는 크기와 데이터 type가 동일해야 한다는 것이다.\n",
        "3. **tf.RaggedTensor**\n",
        "  - list의 list를 나타낸다. \n",
        "  - tensor에 포함된 값은 동일한 data type를 가져야 하지만 list의 길이는 다를 수 있다.\n",
        "  - tf.ragged 패키지는 이러한 텐서를 위한 연산을 제공해 준다.\n",
        "4. **string tensor**\n",
        "  - tf.string 타입의 tensor이다.\n",
        "  - 이는 unicode가 아니라 byte string을 나타내기 때문에 자동으로 UTF-8로 인코딩한다.\n",
        "  - 따라서 tf.int32 텐서를 이용해서 unicode로 바꿔주거나 tf.strings 패키지를 이용해서 텐서의 type를 바꿔 주어야 한다.\n",
        "5. **set**\n",
        "  - 집합은 일반적인 텐서를 나타낸다.\n",
        "  ```tf.constant([[1,2],[3,4]])```는 두개의 집합 {1,2},{3,4}를 나타낸다.\n",
        "  - tf.sets 패키지의 연산을 이용해서 다룰 수 있다.\n",
        "6. **queue**\n",
        "  - 단계별로 tensor를 저장한다.\n",
        "  - FIFOqueue, PriorityQueue, RandomShuffleQueue, PaddingFIFOQueue 등이 존재한다.         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHYX3ag-ZGJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "51efb50a-4ee4-4ed7-e662-db0a16f9b550"
      },
      "source": [
        "tf.constant([[1,2], [3,4]])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[1, 2],\n",
              "       [3, 4]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Zb3ZO5bxlQ",
        "colab_type": "text"
      },
      "source": [
        "#### 5. keras.losses.Loss\n",
        "  - 사용자 정의 손실 함수가 일부 매개 변수를 지원해야 한다면 keras.losses.Loss클래스를 상속하고 __init__()와 call()메서드를 구현해야 한다.\n",
        "  - 손실함수의 매개변수를 모델과 함께 저장하려면 get_config()메서드도 구현해 주어야 한다.  \n",
        "\n",
        "#### 6. keras.metrics.Metric  \n",
        "#### 7. keras.layers.Layer  \n",
        "#### 8. keras.models.Model\n",
        "#### 9. Layer Normalization(층 정규화)를 수행하는 사용자 정의 층 구현하기\n",
        "1. build() 메서드에서 두개의 훈련 가능한 가중치 alpha, betha를 정의한다. 이 두 가중치의 크기가 input_shape[-1:]이고 데이터 type는 tf.float32이다. alpha는 1, betha는 0로 초기화 되어야 한다.\n",
        "2. call() 메서드는 샘플의 특성마다 평균과 표준편차를 계산해야 한다. 이를 위해 전체 샘플의 평균과 분산을 반환하는 tf.nn.moments(inputs, axes = -1, keepdims = True)를 사용할 수 있다. \n",
        "3. 사용자 정의 층이 keras.layers.LayerNoemalization 층과 동일한 출력을 만드는지 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raeNTXkPzp3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUtL9MfT4crI",
        "colab_type": "text"
      },
      "source": [
        "- 아래 만든 class를 살펴보면 __init__, 즉 생성자를 이용해서 모든 하이퍼파라미터를 매개변수로 받는다.\n",
        "  - 이는 python 차원에서 알아볼 때에 파이썬 객체 초기와 initialization method이다.\n",
        "  - **kwargs 매개변수를 추가하는 것 또한 매우 중요하다.\n",
        "\n",
        "- 그리고 밑에 있는 build() 메서드의 역할은 가중치마다 add_weight() 메서드를 호출하여 층의 변수를 만드는 것이다.\n",
        "  **build() 메서드 끝에서는 반드시 부모의 build() 메서드를 호출해야 한다. 이를 통해서 층이 만들어졌음은 keras가 인식하기 때문이다. (self.built = True 로 설정)**\n",
        "\n",
        "- 마지막의 call() 메서드는 이 층에 필요한 연산을 수행한다. 아래 코드의 경우에는 tf.nn.moments()를 이용해서 구한 평균과 분산으로 계산을 수행하게 된다.  \n",
        "\n",
        "- 아래 포함된 것은 아니지만 ```compute_output_shape()```라는 메서드는 이 층의 출력 크기를 반환한다. \n",
        "- 그리고 ```get_config()```라는 메서드는 모든 변수들을 모델과 함께 저장하도록 하는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDtp33LUfdUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "  def __init__(self, activation = None, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.alpha = self.add_weight(shape=input_shape[-1:], initializer=\"ones\", trainable=True)\n",
        "    self.betha = self.add_weight(shape = input_shape[-1:], initializer = \"zeros\", trainable = True)\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    mean, variance = tf.nn.moments(x, axes = -1, keepdims = True)\n",
        "    return self.alpha * (x-mean) / (tf.sqrt(variance + 0.001)) + self.betha\n",
        "\n",
        "layer_norm = LayerNormalization()  "
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLNIwoXJ3_Qm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "f7bb08f8-2ed0-4b20-e0c8-1201541ccaa4"
      },
      "source": [
        "layer_norm(test)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
              "array([[-0.99998,  0.99998],\n",
              "       [-0.99998,  0.99998],\n",
              "       [-0.99998,  0.99998],\n",
              "       [-0.99998,  0.99998],\n",
              "       [-0.99998,  0.99998]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZD6ol6T1bFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "4d844a2a-ba6d-4ee3-acb4-a4e777902dc6"
      },
      "source": [
        "test = tf.constant(np.arange(10).reshape(5,2)*10, dtype = tf.float32)\n",
        "layer_norm(test), real_layer_norm(test)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
              " array([[-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
              " array([[-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998],\n",
              "        [-0.99998,  0.99998]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBHYRMez2aGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "30dcb25e-fbac-4bc4-e155-229295fafb58"
      },
      "source": [
        "layer_norm(x)[0,0,:]"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(28,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFPB4QCW7buQ",
        "colab_type": "text"
      },
      "source": [
        "**실제 LayerNormalization 함수를 이용해서 추출한 값과 내가 만든 layer_norm layer의 값의 mean_absolute_error을 계산해서 어느 정도의 정확도를 갖고 있는지 확인하고자 한다.**\n",
        "\n",
        "- 일단은 alpha, betha 두 가중치를 각각 1, 0으로 초기화한 상태에서의 오차를 확인해 보았다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KVFcGW6o7WO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = x_train.astype(np.float32)\n",
        "\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "real_layer_norm = LayerNormalization(axis = -1)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbMrvLDQwAxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c56aeb5-d5d5-4d68-eadc-0677bb95d7de"
      },
      "source": [
        "tf.reduce_mean(tf.keras.losses.mean_absolute_error(layer_norm(x), real_layer_norm(x)))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=9.2733465e-08>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEkx98NS9AHY",
        "colab_type": "text"
      },
      "source": [
        "  - 두번째는 alpha, betha의 두 가중치를 랜덤하게 지정해서 가중치에 set_weight() 메서드를 이용했고, 다시 오차를 계산해 보았다.\n",
        "    - 두 경우 모두 무시할 만한 오차가 나왔기 때문에 layer가 어느정도 구현이 잘 되었다고 볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJspKq1QuyQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_alpha, random_beta = np.random.rand(x.shape[-1]), np.random.rand(x.shape[-1])\n",
        "layer_norm.set_weights([random_alpha, random_beta])\n",
        "real_layer_norm.set_weights([random_alpha, random_beta])\n"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jopYu1aI1Cjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6091e924-af1f-43d4-8ce7-b8ca7185cd67"
      },
      "source": [
        "tf.reduce_mean(tf.keras.losses.mean_absolute_error(layer_norm(x), real_layer_norm(x)))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=4.7043056e-08>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvtDp9Ck9hAv",
        "colab_type": "text"
      },
      "source": [
        "#### 10. 사용자 정의 훈련 반복을 이용해서 fashion_MNIST 데이터셋으로 모델을 훈련해 보고자 한다.\n",
        "- 사실 웬만해서는 fit()를 이용해서 훈련을 하는 것이 좋기는 하지만, 정말 극도의 유연성을 필요로 하는 경우가 아니고서는 쓰지 않는 것이 좋다. \n",
        "  - 그래도 한번 구현해 보는 것도 나쁘지 않으니 코드를 짜 볼 생각이다.\n",
        "\n",
        "- 사용자 정의 훈련 반복을 하려면 model을 만든 이후에 compile을 할 필요가 없다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "junsaKP4Amnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.3)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6vMl1S8Axme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a8be198-5071-4a37-911b-14b2e7221ae4"
      },
      "source": [
        "x_train.shape, x_val.shape, x_test.shape"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((42000, 28, 28), (18000, 28, 28), (10000, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuos9NBH1JaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(128, kernel_size = (3,3), strides = 2, padding = 'same',input_shape = [28,28,1]),\n",
        "    tf.keras.layers.MaxPool2D(3),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "])\n"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dybMMS7h-Rbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "3572abb7-5876-4dd9-84b7-c1296a3a9e75"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 128)       1280      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               204900    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 207,702\n",
            "Trainable params: 207,446\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJEfJIIoA5cK",
        "colab_type": "text"
      },
      "source": [
        "- 사용자 정의 반복 함수를 구현하기 위해서는 함수를 compile하고 fit하는 함수를 불렀을 때에 어떻게 작용을 하는지 알아야 한다.\n",
        "  1. 훈련 세트에서 sample batch를 random하게 선택하는 함수\n",
        "  2. 현재 step 수, 전체 step횟수, epoch시작, 평균 손실 등을 출력하는 함수\n",
        "  3. 몇개의 hyperparmeter을 지정하고 optimizer, loss function등을 정해야 함\n",
        "  4. 훈련 하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxNlDotYO5hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n",
        "x_val = x_val/255.0"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmrQGIa3Ae_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_batch(image, label, batch_size):\n",
        "  idx = np.random.randint(len(image), size = batch_size)\n",
        "  return image[idx], label[idx]"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlDwa2nODBir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_process(iteration, total, loss, metrics = None):\n",
        "  metrics = \"-\".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss]+(metrics or [])])\n",
        "  end = \"\" if iteration < total else \"\\n\"\n",
        "  print(\"\\r{}/{} - \".format(iteration, total) + metrics, end = end)"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSJH35NxDnDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 32\n",
        "n_steps = len(x_train)//batch_size\n",
        "optimizer = tf.keras.optimizers.Nadam(lr=0.01)\n",
        "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
        "mean_loss = tf.keras.metrics.Mean()\n",
        "#metric는 우리가 model.compile()을 진행할 때에 metrics = ['accuracy']이런식으로 진행하는, 훈련 과정에서 계속 계산할 값을 의미한다.\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXI6KcLJPYce",
        "colab_type": "text"
      },
      "source": [
        "```model.trainable_variables```를 불러오면 해당 model내에 있는 훈련 가능한 변수의 수치를 모두 불러 올 수 있다. 즉, 설정에 따라 다를 수 있지만 weight, bias등의 값을 불러 올 수 있는데 일반적으로 신경망에서는 학습을 할때 역전파 알고리즘과 손실값을 이용해서 trainable_variables를 갱신한다.\n",
        "  - 이는 gradient값을 이용하는데, 갱신 하려는 값으로 손실을 미분하면 그 값에 따라서 변동 추이를 결정한다.\n",
        "- ```model.losses```를 불러오면 해당 모델이 계산한 loss를 모두 불러 올 수 있다. 각 parameter마다 손실을 정방향으로 계산했을 것이기 때문에 그 값이 필요하다.\n",
        "- ```tf.keras.metrics```는 정확도와 손실 같이 현재 학습의 수준이나 정도를 판단할 수 있는 다양한 수치들이다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nUKQK7MEJD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "1c9450ff-48ec-4512-c072-b5a2555b5b45"
      },
      "source": [
        "for i in range(1, epochs + 1):\n",
        "  print('Epoch {}/{}'.format(i, epochs))\n",
        "  for j in range(1, n_steps+1):\n",
        "    x_batch, y_batch = random_batch(x_train, y_train, batch_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = model(x_batch, training = True)\n",
        "      main_loss = tf.reduce_mean(loss_fn(y_batch, output))\n",
        "      loss = tf.add_n([main_loss] + model.losses)\n",
        "    #여기서 분명히 가중치와 편향으로 loss를 미분해야 하는데 그 값을 model.trainable_variables로 불러오면 되는 것이었다.\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #optimizer에 의해서 gradinet 값을 바탕으로 모델의 가중치와 편향이 수정이 되는데, 따라서 여기서도 gradient와 model.trainable_variables를 묶어서 반환한다.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    #전체 batch들의 손실을 계산한 값의 평균이다.\n",
        "    mean_loss(loss)\n",
        "    for metric in metrics:\n",
        "      metric(y_batch, output)\n",
        "    print_process(j*batch_size, len(y_train), mean_loss, metrics)\n",
        "  print_process(len(y_train), len(y_train), mean_loss, metrics)\n",
        "  for metric in [mean_loss]+metrics:\n",
        "    #모든 metric의 변수들을 초기화해준다.\n",
        "    metric.reset_states()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 - mean: 0.5061-sparse_categorical_accuracy: 0.8217\n",
            "60000/60000 - mean: 0.5061-sparse_categorical_accuracy: 0.8217\n",
            "Epoch 2/10\n",
            "60000/60000 - mean: 0.3846-sparse_categorical_accuracy: 0.8571\n",
            "60000/60000 - mean: 0.3846-sparse_categorical_accuracy: 0.8571\n",
            "Epoch 3/10\n",
            "60000/60000 - mean: 0.3730-sparse_categorical_accuracy: 0.8635\n",
            "60000/60000 - mean: 0.3730-sparse_categorical_accuracy: 0.8635\n",
            "Epoch 4/10\n",
            "60000/60000 - mean: 0.3702-sparse_categorical_accuracy: 0.8637\n",
            "60000/60000 - mean: 0.3702-sparse_categorical_accuracy: 0.8637\n",
            "Epoch 5/10\n",
            "60000/60000 - mean: 0.3593-sparse_categorical_accuracy: 0.8718\n",
            "60000/60000 - mean: 0.3593-sparse_categorical_accuracy: 0.8718\n",
            "Epoch 6/10\n",
            "60000/60000 - mean: 0.3476-sparse_categorical_accuracy: 0.8747\n",
            "60000/60000 - mean: 0.3476-sparse_categorical_accuracy: 0.8747\n",
            "Epoch 7/10\n",
            "60000/60000 - mean: 0.3353-sparse_categorical_accuracy: 0.8794\n",
            "60000/60000 - mean: 0.3353-sparse_categorical_accuracy: 0.8794\n",
            "Epoch 8/10\n",
            "60000/60000 - mean: 0.3458-sparse_categorical_accuracy: 0.8763\n",
            "60000/60000 - mean: 0.3458-sparse_categorical_accuracy: 0.8763\n",
            "Epoch 9/10\n",
            "60000/60000 - mean: 0.3257-sparse_categorical_accuracy: 0.8808\n",
            "60000/60000 - mean: 0.3257-sparse_categorical_accuracy: 0.8808\n",
            "Epoch 10/10\n",
            "60000/60000 - mean: 0.3277-sparse_categorical_accuracy: 0.8826\n",
            "60000/60000 - mean: 0.3277-sparse_categorical_accuracy: 0.8826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlOapyl4V7pE",
        "colab_type": "text"
      },
      "source": [
        "- 어느 정도 괜찮은 수치로 학습이 된 것 같다. 역시나 Dense layer만 사용할 때보다 image데이터를 다룰 때에는 convolution layer이 필수 인 것 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf5uIbHIWPNS",
        "colab_type": "text"
      },
      "source": [
        "#### 2. 마지막으로 상위층과 하위층의 학습률이 다른 optimizer을 사용해 보라는 조건이 있다.\n",
        "- 일반적으로는 전체 모델이 있으면 optimizer은 일치하는데 다르게 하라는 요구이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yvusykFFH6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbt9Om7FeECY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower_layer = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(128, input_shape = [28,28,1], kernel_size = (2,2), strides = 2, padding = 'same',activation = 'relu'),\n",
        "  tf.keras.layers.MaxPool2D(3),\n",
        "  tf.keras.layers.BatchNormalization()\n",
        "])\n",
        "upper_layer = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(100, activation = 'relu'),\n",
        "  tf.keras.layers.Dense(10, activation = 'softmax')                                       \n",
        "])\n",
        "model = tf.keras.models.Sequential([lower_layer, upper_layer])\n"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVS_2t4fhMM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "5d9ea727-3039-49dc-82ee-b4d1a165ae3d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      (None, 4, 4, 128)         1152      \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 10)                205910    \n",
            "=================================================================\n",
            "Total params: 207,062\n",
            "Trainable params: 206,806\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRupQpVmholo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower_optimizer = tf.keras.optimizers.Adam(lr=0.002)\n",
        "upper_optimizer = tf.keras.optimizers.Nadam(lr=1e-3)"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9eFAJFIh7sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "n_steps = len(x_train)//batch_size\n",
        "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
        "mean_loss = tf.keras.metrics.Mean()\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3mtt0UVvX02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train[:5000]\n",
        "y_train = y_train[:5000]\n",
        "x_val = x_val[:1000]\n",
        "y_val = y_val[:1000]"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmgPXxJfihzQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "3f49ed67-0490-497b-eb77-151acba4642e"
      },
      "source": [
        "for i in range(1, epochs+1):\n",
        "  print('Epoch {}/{}'.format(i, epochs))\n",
        "  for j in range(1, n_steps+1):\n",
        "    #역시나 batch_size에 맞게 샘플 데이터를 선택했다\n",
        "    x_batch, y_batch = random_batch(x_train, y_train, batch_size)\n",
        "\n",
        "    #gradient tape를 이용해서 원래 신경망이 역전파 알고리즘을 이용해서 구현했었을 가중치와 편향에 대한 손실의 미분값을 구했다.\n",
        "    with tf.GradientTape(persistent = True) as tape:\n",
        "      pred = model(x_batch)\n",
        "      main_loss = tf.reduce_mean(loss_fn(y_batch, pred))\n",
        "      loss = tf.add_n([main_loss] + model.losses)\n",
        "\n",
        "    #optimizer을 두개를 설정했으니 각각에 대해서 gradinet를 구하고 적용해 주면 된다.\n",
        "    for layers, optimizer in ((lower_layer, lower_optimizer), (upper_layer, upper_optimizer)):\n",
        "      gradients = tape.gradient(loss, layers.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
        "    \n",
        "    #지금까지 구한 손실의 평균을 내준다.\n",
        "    mean_loss(loss)\n",
        "\n",
        "    status = dict()\n",
        "    #mean_loss.result().numpy()를 해 주어야 결괏값을 수치로 입력 받을 수 있다. \n",
        "    status['loss'] = mean_loss.result().numpy()\n",
        "\n",
        "    for metric in metrics:\n",
        "      #metric을 이용해서 예측값과 실제 값 사이의 차이, accuracy, loss등의 다양한 수치들을 계산한다.\n",
        "      metric(y_batch, pred)\n",
        "      status[metric.name] = metric.result().numpy()\n",
        "    \n",
        "    val_pred = model(x_val)\n",
        "    status[\"val_loss\"] = np.mean(loss_fn(y_val, val_pred))\n",
        "    status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(tf.constant(y_val, dtype = tf.float32), val_pred))\n",
        "\n",
        "    print_process(j*batch_size, len(y_train), mean_loss, metrics)\n",
        "  print_process(len(y_train), len(y_train), mean_loss, metrics)\n",
        "  #validation data의 val_loss와 val_accuracy를 구해준다.\n",
        "  print('val_loss = {}, val_accuracy = {}'.format(status[\"val_loss\"], status['val_accuracy']))\n",
        "\n",
        "  #metric의 모든 변수 초기화\n",
        "  for metric in metrics + [mean_loss]:\n",
        "    metric.reset_states()\n",
        "\n",
        "\n"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5000/5000 - mean: 1.0200-sparse_categorical_accuracy: 0.6480\n",
            "val_loss = 3.0052783489227295, val_accuracy = 0.10100000351667404\n",
            "Epoch 2/5\n",
            "5000/5000 - mean: 0.5694-sparse_categorical_accuracy: 0.7965\n",
            "val_loss = 3.1855366230010986, val_accuracy = 0.10100000351667404\n",
            "Epoch 3/5\n",
            "5000/5000 - mean: 0.4895-sparse_categorical_accuracy: 0.8225\n",
            "val_loss = 3.542616844177246, val_accuracy = 0.10100000351667404\n",
            "Epoch 4/5\n",
            "5000/5000 - mean: 0.4354-sparse_categorical_accuracy: 0.8435\n",
            "val_loss = 3.3981246948242188, val_accuracy = 0.09200000017881393\n",
            "Epoch 5/5\n",
            "5000/5000 - mean: 0.4176-sparse_categorical_accuracy: 0.8488\n",
            "val_loss = 3.6849002838134766, val_accuracy = 0.10100000351667404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8MH4QR9v429",
        "colab_type": "text"
      },
      "source": [
        "- 직접 가중치 갱신 및 gradient연산, batch size에 맞는 데이터 생성, 그리고 진행 경로 출력등 compile과 fit을 모두 함수로 직접 구현해 보는 과정을 거치니 훨씬 더 제대로 신경망의 학습 과정을 이해할 수 있었던 것 같다.\n",
        "- 그리고 class를 잘 이해를 못했었는데 그래도 Layer Normalization이라는 모듈을 직접 class로 구현해 봄으로서 편하게 쓰던 다른 keras API도 직접 class로 구현해 보면 재밌을 것 같았다.\n",
        "- 또한 tensor의 자료형이 헷갈렸던 부분이 많았는데 어느정도 이번 공부를 하면서 배우는 게 많은 것 같다."
      ]
    }
  ]
}