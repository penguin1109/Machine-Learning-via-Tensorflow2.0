{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어 생성",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4Q1BzUfpr-",
        "colab_type": "text"
      },
      "source": [
        "### 1. 한글 원본 텍스트를 자소 단위와 단어 단위로 나누어서 순환 신경망으로 생성해 보고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA3gzT33f3rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdwtLkPGinrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = open('/content/sample_data/california_housing_train.csv', 'rb').read().decode(encoding = 'utf-8')\n",
        "df = df.split('\\n')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A00Wouq1lYBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d9288ec-f06e-4b45-dec6-0d4c6e5cc67b"
      },
      "source": [
        "df[1].split(',')[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'-114.310000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hktca1PFiyMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "for i in df[0].split(','):\n",
        "  labels.append(i)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpx9wR2IkcPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = []\n",
        "for i in df:\n",
        "  file.append(i.split(','))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6T6GpKOk6BZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "030b8a24-7dd8-4952-b2ca-fe2b9f505f07"
      },
      "source": [
        "file[len(file)-2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-124.350000',\n",
              " '40.540000',\n",
              " '52.000000',\n",
              " '1820.000000',\n",
              " '300.000000',\n",
              " '806.000000',\n",
              " '270.000000',\n",
              " '3.014700',\n",
              " '94600.000000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzeSNvX2kqoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = dict()\n",
        "for i in range(len(labels)):\n",
        "  temp = []\n",
        "  for k in range(1,len(file)-1):\n",
        "    temp.append(file[k][i])\n",
        "  d[labels[i]] = temp"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH0JkS-uoLBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame(d)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdATFZX7oPYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "bf7f410b-a6dd-4fcc-fd67-4b9f6d85b35d"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>\"longitude\"</th>\n",
              "      <th>\"latitude\"</th>\n",
              "      <th>\"housing_median_age\"</th>\n",
              "      <th>\"total_rooms\"</th>\n",
              "      <th>\"total_bedrooms\"</th>\n",
              "      <th>\"population\"</th>\n",
              "      <th>\"households\"</th>\n",
              "      <th>\"median_income\"</th>\n",
              "      <th>\"median_house_value\"</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.310000</td>\n",
              "      <td>34.190000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>5612.000000</td>\n",
              "      <td>1283.000000</td>\n",
              "      <td>1015.000000</td>\n",
              "      <td>472.000000</td>\n",
              "      <td>1.493600</td>\n",
              "      <td>66900.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.470000</td>\n",
              "      <td>34.400000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>7650.000000</td>\n",
              "      <td>1901.000000</td>\n",
              "      <td>1129.000000</td>\n",
              "      <td>463.000000</td>\n",
              "      <td>1.820000</td>\n",
              "      <td>80100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.560000</td>\n",
              "      <td>33.690000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>720.000000</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>333.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>1.650900</td>\n",
              "      <td>85700.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.570000</td>\n",
              "      <td>33.640000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>1501.000000</td>\n",
              "      <td>337.000000</td>\n",
              "      <td>515.000000</td>\n",
              "      <td>226.000000</td>\n",
              "      <td>3.191700</td>\n",
              "      <td>73400.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.570000</td>\n",
              "      <td>33.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>1454.000000</td>\n",
              "      <td>326.000000</td>\n",
              "      <td>624.000000</td>\n",
              "      <td>262.000000</td>\n",
              "      <td>1.925000</td>\n",
              "      <td>65500.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   \"longitude\" \"latitude\"  ... \"median_income\" \"median_house_value\"\n",
              "0  -114.310000  34.190000  ...        1.493600         66900.000000\n",
              "1  -114.470000  34.400000  ...        1.820000         80100.000000\n",
              "2  -114.560000  33.690000  ...        1.650900         85700.000000\n",
              "3  -114.570000  33.640000  ...        3.191700         73400.000000\n",
              "4  -114.570000  33.570000  ...        1.925000         65500.000000\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi_YyxmXoT3J",
        "colab_type": "text"
      },
      "source": [
        "- 아무 데이터나 이용해 보았는데, csv파일 또한 **파이썬의 파일 입출력 기능인 open으로 이용이 가능**하다(저장 경로만 알고 있으면)\n",
        "  - 그런데 pandas dataframe의 경우에는 dict파일로 변환된 자료를 바로 바꿀 수 있고, 아니라면 \n",
        "    1. DataFrame.from_records\n",
        "      :Constructor from tuples, also record arrays.\n",
        "\n",
        "    2. DataFrame.from_dict\n",
        "      :From dicts of Series, arrays, or dicts.\n",
        "\n",
        "    3. read_csv\n",
        "      :Read a comma-separated values (csv) file into DataFrame.\n",
        "\n",
        "    4. read_table\n",
        "      :Read general delimited file into DataFrame.\n",
        "\n",
        "    5. read_clipboard\n",
        "        :Read text from clipboard into DataFrame.\n",
        "\n",
        "    이 5가지 방법도 존재한다.\n",
        "\n",
        "-데이터를 다룰떄 자연어 처리같은 작업이 아니라면 저렇게 형태를 바꿔주면 좋다.\n",
        "\n",
        "- 파이썬의 open 기능은 'r', 'w','rd'등 다양한데, 순서대로 읽기, 쓰기, 이진 파일로 받기 등의 방법이다.\n",
        "  - 그러나 일단 텍스트 파일로 받게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jnIc1mCf0-4",
        "colab_type": "text"
      },
      "source": [
        "#### 1. 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvAX4xEsfkA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d23a8b4b-dc13-4c2f-a063-b72e0af4e91d"
      },
      "source": [
        "path = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://bit.ly/2Mc3SOV\n",
            "62013440/62012502 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6T89yZcgT_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#데이터를 메모리에 불러오는데 우선 인코딩 형식으로 'utf-8'을 사용하기로 한다.\n",
        "train_text = open(path, 'rb').read().decode(encoding = 'utf-8')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDYUuRcsNib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5190fb7e-68c9-4f7c-d99a-66c65c38b7cc"
      },
      "source": [
        "train_text[:100]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ufeff태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \\n태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl7VzP9_hq9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#데이터 정제를 위한 함수 만들기\n",
        "#우선 영문은 없고 한글만 있을 것이기 떄문에(조선 왕조 실록 데이터임)\n",
        "import re\n",
        "\n",
        "def clean_str(string):\n",
        "  string = re.sub(r'[^가-힣A-Za-z0-9(),!?\\'\\`]', ' ', string)\n",
        "  string = re.sub(r'\\'ll',' \\ll',string)\n",
        "  string = re.sub(r',', ' , ',string)\n",
        "  string = re.sub(r'!', ' ! ',string)\n",
        "  string = re.sub(r'\\?', ' ? ',string)\n",
        "  string = re.sub(r'\\)', '',string)\n",
        "  string = re.sub(r'\\(','',string)\n",
        "  string = re.sub(r'\\s{2,}',' ',string)\n",
        "  string = re.sub(r'\\`','',string)\n",
        "  string = re.sub(r'\\'{2,}','\\'',string)\n",
        "\n",
        "  return string"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW6XzbJxrgSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = train_text.split('\\n')  #줄별로 띄어서 인식\n",
        "train_text = [clean_str(i) for i in train_text]\n",
        "train_x = []\n",
        "for i in train_text:\n",
        "  train_x.extend(i.split(' '))\n",
        "  train_x.append('\\n')\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kBoo_Drr4Ay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "7844fe59-1479-4622-e6a1-c14984ee0000"
      },
      "source": [
        "train_x = [i for i in train_x if i != '']\n",
        "train_x[:20]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['태조',\n",
              " '이성계',\n",
              " '선대의',\n",
              " '가계',\n",
              " '목조',\n",
              " '이안사가',\n",
              " '전주에서',\n",
              " '삼척',\n",
              " '의주를',\n",
              " '거쳐',\n",
              " '알동에',\n",
              " '정착하다',\n",
              " '\\n',\n",
              " '태조',\n",
              " '강헌',\n",
              " '지인',\n",
              " '계운',\n",
              " '성문',\n",
              " '신무',\n",
              " '대왕']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4HHfpqdth3V",
        "colab_type": "text"
      },
      "source": [
        "#### 2. 단어 토큰화\n",
        "- 한가지 헷갈리고 있었던 사실은 토큰화라는 것이 단어 벡터를 형성해 주는 것이라고 생각했던 것이다. \n",
        "  - 하지만 그것은 원핫 인코딩과 임베딩층에 의해 진행되는 것이다.\n",
        "  - 우리가 자연어 처리를 위해 데이터를 크롤링하거나 수집했을 떄 목적에 맞게 바꿔줄 필요가 있고, 이를 데이터 전처리라고 한다.\n",
        "    - 그 예시로 Tokenization, Cleaning, Normalization이 있는데 Cleaning은 말 그대로 불필요한 문자 등을 문장/문자열 단위로 제거해주는 것이고, Tokenization은 우리가 사용할 의미있는 단위별로 나누어서 구성하는 것을 의미한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HquBxksetbC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49289b4f-23f7-4a96-de4c-7d1d0de7d5db"
      },
      "source": [
        "#단어의 set를 만든다\n",
        "#모든 데이터의 단어들을 입력하다 보니 겹치는 것이 있을 수 있어 그것을 없애주기 위해서 set에 넣는다.\n",
        "word = sorted(set(train_x))\n",
        "word.append('UNK')\n",
        "\n",
        "#단어 리스트를 숫자로 매핑하고, 반대도 실행한다.\n",
        "word2idx = {u:i for i,u in enumerate(word)}\n",
        "idx2word = np.array(word)\n",
        "\n",
        "text_as_int = np.array([word2idx[i] for i in train_x])\n",
        "\n",
        "print(text_as_int[:10])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[299333 229662 161471  17456 111055 230320 251109 155115 225490  29053]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCk9dq_p7kh4",
        "colab_type": "text"
      },
      "source": [
        "- 위의 출력된 값을 보면 알 수 있듯이 우리는 데이터의 토큰화를 마무리 했다.\n",
        "- 실제로 ```from tensorflow.keras.preprocessing.text import Tokenizer```을 통해서(저 모듈을 이용하면 정해주는 단어의 개수만큼의 빈도수별로 정렬된 저장소에서 단어를 찾아 인덱싱을 한다) 토큰화를 진행했어도 상관 없었겠지만 이 데이터의 경우 단어도 너무 많기 때문에 시간 대비 효율적이지 못하다는 판단이 든다.\n",
        "\n",
        "- 'UNK'라는 값을 삽입한 이유는 나중에 학습을 할 때 저장되지 않은 단어를 위한 자리이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrLn7T_488yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "83bd8a47-7484-43b6-e441-fe960824bebe"
      },
      "source": [
        "len(text_as_int), len(word)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6834919, 332669)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7exl8wm9Efc",
        "colab_type": "text"
      },
      "source": [
        "- 총 사용된 단어의 개수는(중복 없이) 332669개이지만 중복을 포함한다면 전체 데이처에 담긴 토큰화된 단어의 개수는 6834919개이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ7qBJSU7gXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c8b4c105-cb4c-43a8-e5a4-7b851e503042"
      },
      "source": [
        "print(train_x[:20])\n",
        "print(text_as_int[:20])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
            "[299333 229662 161471  17456 111055 230320 251109 155115 225490  29053\n",
            " 190323 256157      0 299333  25650 273581  36173 164024 180494  84439]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lqYVSFP8hY4",
        "colab_type": "text"
      },
      "source": [
        "- 개행 문자인 '\\n'는 0으로 변환이 되었음을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8o-cGRd8n-J",
        "colab_type": "text"
      },
      "source": [
        "#### 3. 학습을 위한 데이터셋 만들기\n",
        "- 여기서 학습의 목적을 다시 상기해 보자\n",
        "  - 우리는 일정한 길이의 부분 문장이 주어졌을 때 그 데이터를 완성시킬 수 있도록 학습을 해야 한다.\n",
        "  - 그렇기 때문에 학습 데이터를 만들 때에도 일정 길이의 문장을 학습 데이터로 주고 이어지는 부분을 label,즉 y값으로 입력을 함으로서 학습을 진행해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRLhgTut8ciK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#우리는 한 데이터당(x의 데이터) 문장의 길이를 seq_length라는 변수를 25로 지정해 주는 방법으로 제한하고자 한다.\n",
        "seq_length = 25\n",
        "#batch_size를 설정했을 때 steps_per_epoch를 전체 데이터의 길이에서 batch_size로 나누는것과 같은 상황이다.\n",
        "examples_per_epoch = len(text_as_int) // seq_length\n",
        "sentence_data = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUoYMV9ZD95o",
        "colab_type": "text"
      },
      "source": [
        "- 위와 같이 tensor dataset로 이루어진 데이터의 경우에는 하나씩 골라내어서 numpy()의 형태로 바꿔준 뒤에 어떤 값인지 봐야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JsUSqxQC7eK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "fd2a9b9b-82a1-4248-a029-faa4b58fa761"
      },
      "source": [
        "for i in sentence_data.take(10):\n",
        "  print(i)\n",
        "  print(i.numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(299333, shape=(), dtype=int64)\n",
            "299333\n",
            "tf.Tensor(229662, shape=(), dtype=int64)\n",
            "229662\n",
            "tf.Tensor(161471, shape=(), dtype=int64)\n",
            "161471\n",
            "tf.Tensor(17456, shape=(), dtype=int64)\n",
            "17456\n",
            "tf.Tensor(111055, shape=(), dtype=int64)\n",
            "111055\n",
            "tf.Tensor(230320, shape=(), dtype=int64)\n",
            "230320\n",
            "tf.Tensor(251109, shape=(), dtype=int64)\n",
            "251109\n",
            "tf.Tensor(155115, shape=(), dtype=int64)\n",
            "155115\n",
            "tf.Tensor(225490, shape=(), dtype=int64)\n",
            "225490\n",
            "tf.Tensor(29053, shape=(), dtype=int64)\n",
            "29053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D39IUg0TBeqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "4f1d1721-87a9-4bed-b8e4-b051198b2c00"
      },
      "source": [
        "sentence_data = sentence_data.batch(seq_length + 1, drop_remainder = True)\n",
        "for i in sentence_data.take(1):\n",
        "  print(idx2word[i.numpy()])\n",
        "  print(i.numpy())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',' '휘']\n",
            "[299333 229662 161471  17456 111055 230320 251109 155115 225490  29053\n",
            " 190323 256157      0 299333  25650 273581  36173 164024 180494  84439\n",
            " 224210 164577 230276 210940     28 330342]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MnIPuRcBfhO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "71345d58-e4e7-4cf0-ad8e-0593a5c92ab9"
      },
      "source": [
        "for i in sentence_data.take(1):\n",
        "  print(i)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[299333 229662 161471  17456 111055 230320 251109 155115 225490  29053\n",
            " 190323 256157      0 299333  25650 273581  36173 164024 180494  84439\n",
            " 224210 164577 230276 210940     28 330342], shape=(26,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bskh_7plCfYj",
        "colab_type": "text"
      },
      "source": [
        "- 위에서 tf.data.Dataset으로 텐서의 형태로 바꾸어 주었는데, Dataset에 쓰이는 batch() 함수는 Dataset에서 한번에 반환하는 데이터의 숫자를 지정해 준다.\n",
        "- 그리고 여기서 seq_length+1로 지정을 해 주었는데, 이는 마지막에 정답이 될 1개의 단어를 합쳐서 반환하기 위해서이다.\n",
        "\n",
        "- 이제 이 Dataset으로 ([25개의 단어], 1개의 단어)이런 형태의 데이터로 바뀐 Dataset을 만들어 주어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxFgzYtXCd-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "be877bbd-6d4d-4396-a46f-597ba9874c6a"
      },
      "source": [
        "def split_input_data(data):\n",
        "  return [data[:-1], data[-1]]\n",
        "\n",
        "dataset = sentence_data.map(split_input_data)\n",
        "for i in dataset.take(1):\n",
        "  print(i[0].numpy())  #학습을 입력받는 데이터\n",
        "  print(i[1].numpy())  #학습의 예측의 결과가 되어야 하는 값"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[299333 229662 161471  17456 111055 230320 251109 155115 225490  29053\n",
            " 190323 256157      0 299333  25650 273581  36173 164024 180494  84439\n",
            " 224210 164577 230276 210940     28]\n",
            "330342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5gg72nHJV_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 600\n",
        "steps_per_epoch = len(dataset)//batch_size\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZOh_0haND7T",
        "colab_type": "text"
      },
      "source": [
        "- 빠른 학습을 위해서 한번에 128개의 데이터를 학습하도록 했고, 데이터를 섞을 때의 buffer_size를 10000으로 설정을 했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XcRVFgTNT5f",
        "colab_type": "text"
      },
      "source": [
        "#### 4. 모델 만들기 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_okX39jJzA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "f70a2461-9c30-443e-acee-6faa81ba0c0b"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "#Embedding Layer안의 변수들은 (input_dim, output_dim, input_length) 이다.\n",
        "model.add(tf.keras.layers.Embedding(len(word), 100, input_length = 25))\n",
        "model.add(tf.keras.layers.LSTM(units = 100, return_sequences=True))\n",
        "model.add(tf.keras.layers.Dropout(rate = 0.2))\n",
        "model.add(tf.keras.layers.LSTM(units = 100))\n",
        "model.add(tf.keras.layers.Dense(len(word2idx), activation = 'softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 25, 100)           33266900  \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 25, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 25, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 332669)            33599569  \n",
            "=================================================================\n",
            "Total params: 67,027,269\n",
            "Trainable params: 67,027,269\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16wBLfgFSOPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodel(epoch, logs):\n",
        "  if epoch%5 != 0 and epoch != 49:\n",
        "    return\n",
        "  test_sentence = train_text[0]\n",
        "  next_words = 100\n",
        "  for _ in range(next_words):\n",
        "    x_test = test_sentence.split(' ')[-seq_length:]\n",
        "    x_test = np.array([word2idx[i] if i in word2idx else word2idx['UNK']for i in x_test])\n",
        "    x_test = pad_sequences([x_test], maxlen = seq_length, padding = 'post', value = word2idx['UNK'])\n",
        "    output = model.predict_classes(x_test)\n",
        "    #그냥 predict()를 하게 되면 출력값의 개수가 전체 단어 개수인 332640개이므로 predict_classes를 이용해서 출력값이 가장 큰 인덱스를 이용\n",
        "    test_sentence += ' '+idx2word[output[0]]\n",
        "  print()\n",
        "  print(test_sentence)\n",
        "  print()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6MfK2yJV6oD",
        "colab_type": "text"
      },
      "source": [
        "- tensor dataset으로는 validation split를 적용하기가 어려워서 직접 학습 결과를 확인하기 위해서 testmodel이라는 이름으로 callback 함수를 정의했다.\n",
        "- 임의의 문장을 입력하고, 뒤에서부터 25개의 단어를 선택하고, 이 단어를 word2idx에 저장된 인덱스로 바꾸어주었다. 단, 만약에 저장이 안되어있는 단어라면 'UNK'의 인덱스를 입력하였다. \n",
        "- 그리고 입력한 25길이의 문장에 예측한 뒤의 단어를 덧붙여서 결괏값으로서 출력이 되도록 하였다.\n",
        "- 출력 단어는 test_sentence의 끝부분에 저장되어 다음 스텝의 입력에 활용이 되는데 중요한 것은 학습을 시킬 때에 특히 keras dataset을 이용하기 때문에 데이터의 처음과 끝을 모르는 관계로 .repeat()를 이용해 끊임없이 반복 적용해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqyNh4zuRg2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47c93839-e1ed-4353-e287-bf66dabf694e"
      },
      "source": [
        "testcallback = tf.keras.callbacks.LambdaCallback(on_epoch_end = testmodel)\n",
        "history = model.fit(dataset.repeat(), epochs = 7, steps_per_epoch=steps_per_epoch, callbacks = [testcallback])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "438/438 [==============================] - ETA: 0s - loss: 8.1428 - accuracy: 0.0804WARNING:tensorflow:From <ipython-input-42-37b6b0be43b5>:12: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "\n",
            " 태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  의 의 의 의 의 의 의 의 그 의 을 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "\n",
            "438/438 [==============================] - 249s 569ms/step - loss: 8.1428 - accuracy: 0.0804\n",
            "Epoch 2/7\n",
            "438/438 [==============================] - 245s 559ms/step - loss: 7.9277 - accuracy: 0.0848\n",
            "Epoch 3/7\n",
            "438/438 [==============================] - 244s 558ms/step - loss: 7.7196 - accuracy: 0.0965\n",
            "Epoch 4/7\n",
            "438/438 [==============================] - 244s 558ms/step - loss: 7.5136 - accuracy: 0.1068\n",
            "Epoch 5/7\n",
            "438/438 [==============================] - 244s 556ms/step - loss: 7.2780 - accuracy: 0.1183\n",
            "Epoch 6/7\n",
            "438/438 [==============================] - ETA: 0s - loss: 7.0278 - accuracy: 0.1312\n",
            " 태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  의 의 의 의 의 의 의 이 이 이 이 그 같이 , 그 일을 그 것을 그 것을 그 것을 그 것을 보내어 그 주고 , 그 보내어 보내어 가지고 그 주고 , 그 보내어 가지고 가지고 가지고 보내어 보내어 보내어 가지고 가지고 보내어 보내어 보내어 가지고 가지고 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 보내어 가지고 그 주고 , 그 보내어 보내어 가지고 그 주고 , 그 주고 , 그 주고 , 그 관찰사 을 보내어 그 주고 , 그 보내어\n",
            "\n",
            "438/438 [==============================] - 246s 561ms/step - loss: 7.0278 - accuracy: 0.1312\n",
            "Epoch 7/7\n",
            "438/438 [==============================] - 241s 550ms/step - loss: 6.7942 - accuracy: 0.1428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4oNtStOPytg",
        "colab_type": "text"
      },
      "source": [
        "#### 6. 임의의 문장을 이용해서 생성 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2gTKpY2hTlY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7ecc556-79c9-4be1-9049-c11b5c42bac0"
      },
      "source": [
        "train_text[23]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' 그 짐승이 많은 것을 탐내서 지금까지 돌아오지 않습니다 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0kXvnXLOOwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "64e09b73-4db6-4524-93d6-1b6827864071"
      },
      "source": [
        "sentence = train_text[23]\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "next_word = 100\n",
        "for _ in range(next_word):\n",
        "  test = sentence.split(' ')[-seq_length:]\n",
        "  test = [[word2idx[i] if i in word2idx else word2idx['UNK']]for i in test]\n",
        "  test = pad_sequences([test], maxlen = seq_length, padding = 'post', value = word2idx['UNK'])\n",
        "  pred = model.predict_classes(test)\n",
        "  pred = idx2word[pred[0]]\n",
        "  sentence += ' '+pred\n",
        "print(sentence)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 그 짐승이 많은 것을 탐내서 지금까지 돌아오지 않습니다  의 의 의 의 의 이 이 이 이 이 이 이 한 이 이 , 그 말을 아뢰기를 , \n",
            " 하였다 임금이 말하기를 , \n",
            " 임금이 그 관찰사 을 보내어 명하여 그 주고 , 그 보내어 와서 가지고 와서 가지고 와서 가지고 와서 가지고 와서 가지고 와서 가지고 와서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서 와서 가서 가지고 와서\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6im3LnXcTGS",
        "colab_type": "text"
      },
      "source": [
        "### 2. 자모 단위 생성\n",
        "- 위에서 진행한 작업은 단어 단위로 자연어를 학습하고 생성했다면 이번에는 자모단위로 자연어를 생성하는 연습을 하고자 한다.\n",
        "- 그렇게 하기 위해서는 자모 단위로 분리를 해야 하는데, 이는 이미 만들어져 있는 jamotools라는 라이브러리를 사용하고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs1BjLAthbHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "770d5931-9ad5-481e-cb70-4bae837ba5ea"
      },
      "source": [
        "!pip install jamotools"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jamotools\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/d6/ec13c68f7ea6a8085966390d256d183bf8488f8b9770028359acb86df643/jamotools-0.1.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from jamotools) (0.16.0)\n",
            "Installing collected packages: jamotools\n",
            "Successfully installed jamotools-0.1.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQNh0PwGcU-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jamotools\n",
        "train_text = open(path, 'rb').read().decode(encoding = 'utf-8')\n",
        "\n",
        "train_x = jamotools.split_syllables(train_text)\n",
        "word = sorted(set(train_x))\n",
        "word.append('UNK')\n",
        "\n",
        "char2idx = {u:i for i,u in enumerate(word)}\n",
        "idx2char = np.array(word)\n",
        "\n",
        "#문서의 자모를 나눈 것을 전부 인덱스로 바꾼 데이터\n",
        "text_as_int = np.array([char2idx[i] if i in char2idx else char2idx['UNK']for i in train_x])\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvReSOsheskH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 80\n",
        "#한번에 80의 길이를 가진 자모 모음을 학습 입력값으로 사용할 것임\n",
        "#examples_per_epoch는 한번의 epoch에 학습하는 묶음의 개수\n",
        "examples_per_epoch = len(text_as_int) // seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "#데이터셋을 (x,y)꼴로, 즉 (입력할 seq_length의 길이의 자모 모음, 뒤에 이어질 자모)\n",
        "char_dataset = char_dataset.batch(seq_length+1, drop_remainder = True)\n",
        "#map()함수로 x와 y를 하나의 데이터로 묶어줌\n",
        "char_dataset = char_dataset.map(split_input_data)\n",
        "\n",
        "batch_size = 600\n",
        "bufffer_size = 10000\n",
        "char_dataset = char_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)\n",
        "steps_per_epoch = examples_per_epoch//batch_size\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSI9_BLEgd9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "9a5fbde0-7897-406e-ddbf-e0394de84fa2"
      },
      "source": [
        "#중복 없는 분리된 자모의 개수\n",
        "total_char = len(word)\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(word), 100, input_length = seq_length))\n",
        "model.add(tf.keras.layers.LSTM(units = 100, return_sequences=  True))\n",
        "model.add(tf.keras.layers.Dropout(rate = 0.3))\n",
        "model.add(tf.keras.layers.LSTM(units = 100))\n",
        "model.add(tf.keras.layers.Dense(len(word), activation = 'softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "def test_cb(epoch, logs):\n",
        "  if epoch%2 != 0 and epoch != 6:\n",
        "    return\n",
        "  sentence = train_text[:48]\n",
        "  for _ in range(300):\n",
        "    test = jamotools.split_syllables(sentence)\n",
        "    test = test[-seq_length:]\n",
        "    test = np.array([char2idx[i] if i in char2idx else char2idx['UNK']for i in test])\n",
        "    test = pad_sequences([test], maxlen = seq_length, padding = 'post', value = char2idx['UNK'])\n",
        "\n",
        "    pred = model.predict_classes(test)\n",
        "    sentence += ' '+idx2char[pred[0]]\n",
        "    #여기서 약간 실수가 있었는데 ' '가 아니라 ''으로 더했어야 나중에 자모 합칠때 단어가 된다\n",
        "  print()\n",
        "  print(jamotools.join_jamos(sentence))\n",
        "  print()\n",
        "\n",
        "test_cb = tf.keras.callbacks.LambdaCallback(test_cb)\n",
        "model.fit(char_dataset.repeat(), steps_per_epoch=steps_per_epoch, callbacks=[test_cb], epochs = 7, verbose = 2)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 80, 100)           619800    \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 80, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 80, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6198)              625998    \n",
            "=================================================================\n",
            "Total params: 1,406,598\n",
            "Trainable params: 1,406,598\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다  茇 茇 奪 枚 枚 奭 奭 奭 赴 赴 赴 赴 赴 赴 赴 赴 赴 赴 窩 韞 韞 涉 總 總 總 梗 梗 幄 幄 幄 𧞤 𧞤 蛾 蛾 幄 𧞤 𧞤 𧞤 渙 蛾 疹 𧞤 𧞤 𧞤 珇 珇 渙 渙 欲 欲 梗 梗 幄 幄 幄 檝 檝 頑 頑 渙 止 渙 嫉 嫉 疹 幄 梗 幄 幄 幄 蛾 蛾 𧞤 幄 幄 𧞤 𧞤 蛾 蛾 幄 幄 𧞤 𧞤 蛾 蛾 𧞤 幄 𧞤 𧞤 蛾 蛾 𧞤 幄 𧞤 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾 蛾 𧞤 𧞤 幄 𧞤 蛾\n",
            "\n",
            "Epoch 1/7\n",
            "1008/1008 - 77s - loss: 3.7163 - accuracy: 0.1042\n",
            "Epoch 2/7\n",
            "1008/1008 - 78s - loss: 3.1886 - accuracy: 0.1681\n",
            "\n",
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다  ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ   ㅇ ㅣ  \n",
            "\n",
            "Epoch 3/7\n",
            "1008/1008 - 79s - loss: 2.5813 - accuracy: 0.2889\n",
            "Epoch 4/7\n",
            "1008/1008 - 79s - loss: 2.3710 - accuracy: 0.3313\n",
            "\n",
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다  ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ ㅣ ㄴ ㅏ ㅆ ㅏ   ㅇ ㅣ ㅇ\n",
            "\n",
            "Epoch 5/7\n",
            "1008/1008 - 79s - loss: 2.2332 - accuracy: 0.3617\n",
            "Epoch 6/7\n",
            "1008/1008 - 79s - loss: 2.1426 - accuracy: 0.3825\n",
            "\n",
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다  ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ   ㅇ ㅣ ㄴ ㅏ  \n",
            "\n",
            "Epoch 7/7\n",
            "1008/1008 - 79s - loss: 2.0749 - accuracy: 0.4002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fafa05f55f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acj5JVaMn_zJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ce42bc80-6c72-4eca-8789-02fe7d22645b"
      },
      "source": [
        "train_text[300:500]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'복야(僕射) 휘(諱) 이천상(李天祥)을 낳고, 복야가 아간(阿干) 휘(諱) 광희(光禧)를 낳고, 아간이 사도(司徒) 삼중 대광(三重大匡) 휘(諱) 입전(立全)을 낳고, 사도가 휘(諱) 이긍휴(李兢休)를 낳고, 이긍휴가 휘(諱) 염순(廉順)을 낳고, 염순이 휘(諱) 이승삭(李承朔)을 낳고, 이승삭이 휘(諱) 충경(充慶)을 낳고, 충경이 휘(諱) 경영(景英)을'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3mTdLo1kmBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5d630b5f-23c4-428e-b788-bc1a105888b4"
      },
      "source": [
        "sentence = train_text[300:400]\n",
        "sentence = jamotools.split_syllables(sentence)\n",
        "for _ in range(300):\n",
        "  test = sentence[-seq_length:]\n",
        "  test = [char2idx[i] if i in char2idx else char2idx['UNK']for i in test]\n",
        "  test = pad_sequences([test], maxlen = seq_length, padding = 'post', value = char2idx['UNK'])\n",
        "  pred = model.predict_classes(test)\n",
        "  #pred는 숫자로, 즉 예측하는 단어의 index를 출력\n",
        "  sentence += ''+idx2char[pred[0]]\n",
        "print(jamotools.join_jamos(sentence))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "복야(僕射) 휘(諱) 이천상(李天祥)을 낳고, 복야가 아간(阿干) 휘(諱) 광희(光禧)를 낳고, 아간이 사도(司徒) 삼중 대광(三重大匡) 휘(諱) 입전(立全)을 낳고, 사도가 휘(李)를 정상(李山)·이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 이름이 일\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT5Vv5jboxRF",
        "colab_type": "text"
      },
      "source": [
        "- 시간이 너무 오래 걸려서 7번만 학습했지만 학습수를 높이면 충분히 학습이 가능할 것이다.\n",
        "- RNN을 이용할떄는 연속으로 변화가 있어야 함에 주의해야 한다"
      ]
    }
  ]
}